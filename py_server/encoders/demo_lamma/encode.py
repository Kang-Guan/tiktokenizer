# Copyright (c) Meta Platforms, Inc. and affiliates.
# This software may be used and distributed according to the terms of the GNU General Public License version 3.

import os
from logging import getLogger
from typing import List

from sentencepiece import SentencePieceProcessor

logger = getLogger()

current_file_path = os.path.abspath(__file__)
model_path = os.path.join(os.path.dirname(current_file_path), 'tokenizer.model')

class LLaMATokenizer:
    def __init__(self, model_path = model_path):
        # reload tokenizer
        assert os.path.isfile(model_path), model_path
        self.sp_model = SentencePieceProcessor(model_file=model_path)
        logger.info(f"Reloaded SentencePiece model from {model_path}")

        # BOS / EOS token IDs
        self.n_words: int = self.sp_model.vocab_size()
        self.bos_id: int = self.sp_model.bos_id()
        self.eos_id: int = self.sp_model.eos_id()
        self.pad_id: int = self.sp_model.pad_id()
        logger.info(
            f"#words: {self.n_words} - BOS ID: {self.bos_id} - EOS ID: {self.eos_id}"
        )
        assert self.sp_model.vocab_size() == self.sp_model.get_piece_size()

    def encode(self, s: str, bos: bool, eos: bool) -> List[int]:
        assert type(s) is str
        t = self.sp_model.encode(s)
        if bos:
            t = [self.bos_id] + t
        if eos:
            t = t + [self.eos_id]
        return t

    def decode(self, t: List[int]) -> str:
        return self.sp_model.decode(t)


tokenizer = LLaMATokenizer()

def conv(tokens, text):
    '''
    decode æ¯ä¸ªè§£æå‡ºçš„ int tokenï¼Œç„¶åæ‰¾åˆ°åŸå§‹å­—ç¬¦ä¸²ä¸­å¯¹åº”çš„å­—ç¬¦ï¼Œæ‰“åŒ…æˆ Segments List
    æŸäº› emoji æˆ–è€…æ±‰å­—ä¼šä½¿ç”¨å¤šä¸ª token æ¥è¡¨ç¤ºï¼Œéœ€è¦å¦å¤–å¤„ç†

    type Segments = { text: string; tokens: { id: number; idx: number }[] }[]
    '''
    segments = []
    token_acc = []
    for idx in range(len(tokens)):
    
        token_acc.append({'id':tokens[idx], 'idx':idx})
        decoded = tokenizer.decode([x['id'] for x in token_acc])
        
        # å¦‚æœä¸‹ä¸€ä¸ªå­—ç¬¦æ˜¯ç©ºæ ¼ï¼Œä½† decode ç»“æœå¹¶ä¸æ˜¯ç©ºæ ¼å¼€å¤´ï¼Œç»™ decode è¡¥ç©ºæ ¼å»åšå°è¯•
        got = [' ' + decoded, decoded] if len(text) and text[0] == ' ' \
            else [decoded]
        
        hit = False
        for tk in got:
            if hit:
                continue
            tk_len = len(tk)
            # æŸäº› emoji æˆ–è€…æ±‰å­—ä¼šä½¿ç”¨å¤šä¸ª token æ¥è¡¨ç¤º
            # è§£æç»“æœä¸åŸå§‹å­—ç¬¦ä¸²ç›¸åŒæ—¶ï¼Œå¡å…¥ä¸€ä¸ªç»“æœï¼Œå¦åˆ™ç»§ç»­ append å¯¼ token_acc
            txt_to_check = text[:tk_len]
            if tk == txt_to_check:
                segments.append({
                    'text': txt_to_check,
                    'tokens': token_acc.copy()
                })
                text = text[tk_len:]
                token_acc = []
                hit = True

    return segments

def encode(text):
    ids = tokenizer.encode(text, False, False)
    return conv(ids, text)
    
if __name__=='__main__':

    def test(text:str):
        result = "".join([seg['text'] for seg in encode(text)])
        print(text)
        print(result)
        assert text==result
        print('------')

    test(' ')
    test('1')
    test('a')
    test('apple')
    test('apple apple apple')
    test('å’šå’šå’šå’š')
    test('åœ¨')
    test('é¥•é¤®')
    test('one two three')
    test(' ğŸ˜ƒ one two three ')
    test('ğŸ˜ƒå’šå’šå’šå™¢çš„é‚£ä¸ª one two three ')
    test(' one two three\n ThreeThreeğŸ˜ƒThree')
    
